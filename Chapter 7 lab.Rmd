**Non- Linear Modelling**

```{r}
library(ISLR)
attach(Wage)
```

**Polynomial Regression**
```{r}
poly.fit = lm(wage~poly(age,4), data=Wage)
coef(summary(poly.fit))
```
Above we are using orthogonal polynomials, which means that each term of polynomial is linear combination of variables age, age^2, age^3, age^4. so we are testing each of coefficient seperately. We can decide seperately for each term if that is needed or not.

we can use also use raw polynomials by:
```{r}
poly.fit2 = lm(wage~poly(age,4,raw=T), data= Wage)
coef(summary(poly.fit2))
```
Choice of type of polynomial does not affect fitted values obtained but it affects coefficient estimate as seen.

we can fit polynomial regression by:

```{r}
poly.fit3 = lm(wage~age+I(age^2)+I(age^3)+I(age^4), data=Wage)
coef(poly.fit3)
```
```{r}
plot(fitted(poly.fit), fitted(poly.fit3))
```
Perfect straight line proves that there is no change in fitted values even if the coefficients have different values duw to nature of polynomial.(Orthogonal or Raw)



we will now create grid of values of `age` for which we want predictions.

```{r}
age.grid = seq(from=range(age)[1], to=range(age)[2])
```
above function will generate grid of 63 age variables from 18 to 80.

```{r}
preds = predict(poly.fit,newdata=list(age=age.grid), se=TRUE)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)

```

```{r}

par(mfrow=c(1,2),mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
```
![Plot Margin](https://www.r-graph-gallery.com/74-margin-and-oma-cheatsheet_files/figure-html/thecode-1.png)


As we can see there are 2 margins. `mar` argument is used to set margins and `oma` to set outer margins.


```{r}

plot(age,wage,xlim=range(age),cex=0.5, col="darkgrey")
title("Degree- 4 Polynomial", outer=T)
lines(age.grid,preds$fit,lwd=2, col="blue")
matlines(age.grid,se.bands, lwd=1, col="blue",lty=3)
```

```{r}

preds2 = predict(poly.fit2, newdata=list(age=age.grid), se=TRUE)
max(abs(preds$fit-preds2$fit))

```
Above quantity suggests that there is very minimal changes is prediction between orthogonal and raw polynomials.


For determing degree of polynomial, we can use hypothesis tests.

We will now fit model ranging from linear to a 5 degree polynomial and will determine the simplest model explaining the relationship between wage and age. 

We will use the `anova()` function which performs analysis of variance (ANOVA using an F-test) in order to test null hypothesis that a model M~1~ is sufficient to explain the data against the alternative hypothesis that a more complex model M~2~ is required.

In order to use `anova()` function, M~1~ and M~2~ must be nested models i.e. all of predictors in M~1~ must be a subset of predictors in M~2~.

```{r}
fit = lm(wage~education, data=Wage)
fit.1 = lm(wage~age, data=Wage)
fit.2 = lm(wage~poly(age,2), data=Wage)
fit.3 = lm(wage~poly(age,3), data=Wage)
fit.4 = lm(wage~poly(age,4), data=Wage)
fit.5 = lm(wage~poly(age,5), data=Wage)
```

using `anova()` function:

```{r}
anova(fit,fit.1,fit.2,fit.3,fit.4,fit.5)
```
P- value comparing linear *model 1* to quadaratic *model 2* is essentialy zero, indicating that a linear fit is not sufficient.
Similarly, P- value comparing quadratic *model 2* to cubic *model 3* is very low(0.0016), so quadratic fit is also insufficient.
The p-value comparing the cubic and degree-4 polynomials, *model 3* and *model 4*, is approximately 5% while the degree-5 polynomial *model 5* seems unnecessary because its p-value is 0.37. Hence, either a cubic or a quartic polynomial appear to provide a reasonable ???t to the data.

we can also obtain P- values briefly by using fact that `poly()` creates orthogonal polynomials.

```{r}
coef(summary(fit.5))
```

We can also use `anova()` method with or without polynomial, it also works when we have other terms in model

```{r}
fit.1 = lm(wage~education+age, data= Wage)
fit.2 = lm(wage~education+poly(age,2), data= Wage)
fit.3 = lm(wage~education+poly(age,3), data= Wage)
anova(fit.1,fit.2,fit.3)
```

We can also choose polynomial using degree validation.


**Polynomial Logistic Regression**

We will now predict whether an individual ears more than $250,000 per year.
```{r}
fit = glm(I(wage>250)~poly(age,3), data= Wage, family = binomial)
summary(fit)
```
we use wrapper `I()` to create binary response. 

```{r}
preds = predict(fit, newdata=list(age=age.grid), se=TRUE)
```

to calculate standard error bands:

```{r}
se.bands = preds$fit + cbind(fit=0, lower=-2*preds$se, upper=2*preds$se)
```
`cbind(fit=0, lower=-2*preds$se, upper=2*preds$se)` will be a 3 column matrix and preds$fit will be added to ech of the columns.
```{r}
se.bands[1:5,]
```
We have done computation on logit scale, to transform we need to apply inverse logit to get it on probability scale.

$$p=\frac{e^\eta}{1+ e^\eta}$$
```{r}
prob.bands = exp(se.bands)/(1+exp(se.bands))
matplot(age.grid,prob.bands, col="blue", lwd=c(2,1,1), lty=c(1,2,2), type="l", ylim=c(0,0.1))
```
We will now add data points to plot.
```{r}
prob.bands = exp(se.bands)/(1+exp(se.bands))
matplot(age.grid,prob.bands, col="blue", lwd=c(2,1,1), lty=c(1,2,2), type="l", ylim=c(0,0.1))
points(jitter(age),I(wage>250)/10, pch="|", cex=0.5)
```

**Splines**

Cubic Splines:

```{r}
require(splines)
fit = lm(wage~bs(age, knots=c(25,40,60)), data=Wage)
plot(age,wage, col="darkgrey")
lines(age.grid, predict(fit, newdata=list(age=age.grid)), col="darkgreen", lwd=2)
abline(v=c(25,40,60), lty=2, col="darkgreen")
```
Smoothing Splines

smoothing splines do not require knot selection , but it has a tuning parameter. Tuning parameter can be specified by effective degrees of freedom(`df`)


```{r}
fit1 = smooth.spline(age, wage, df=16)
plot(age,wage, col="darkgrey")
lines(age.grid, predict(fit, newdata=list(age=age.grid)), col="darkgreen", lwd=2)
abline(v=c(25,40,60), lty=2, col="darkgreen")
lines(fit1,col="red", lwd=2)
```

We can also use LOOCV(Leave one out cross validation) to select the smoothing parameter effectively.

```{r}
fit2 = smooth.spline(age,wage, cv=TRUE)
plot(age,wage, col="darkgrey")
lines(age.grid, predict(fit, newdata=list(age=age.grid)), col="darkgreen", lwd=2)
abline(v=c(25,40,60), lty=2, col="darkgreen")
lines(fit1,col="red", lwd=2)
lines(fit2, col="purple", lwd=2)
```

```{r}
fit2
```

**Generalized Additive Models:**

```{r}
require(gam)
gam1 = gam(wage~s(age,df=4)+s(year, df=4)+ education, data=Wage)
par(mfrow=c(1,3))
plot(gam1, se=TRUE)
```

Using GAM for logistic regression:

```{r}
gam2 = gam(I(wage>250)~s(age,df=4)+s(year,df=4)+education, family=binomial)
plot(gam2)
```

If we want to add a non-linear terms for year:
```{r}
gam2a = gam(I(wage>250)~s(age,df=4)+year+education, data=Wage, family=binomial)
anova(gam2a,gam2,test="Chisq")

```
P- value of 0.82 indicate that we do not need non-linear term for year.

We can use plot method from gam, to plot models fit by `lm` and `glm`.

```{r}
par(mfrow=c(1,3))
lm1 = lm(wage~ns(age,df=4)+ns(year,df=4)+education, data=Wage)
plot.Gam(lm1,se=T)
```
We can also use local regression fits in GAM using `lo()` function

```{r}
gam.lo = gam(wage~s(year,df=4)+lo(age, span=0.7)+education, data=Wage)
plot.Gam(gam.lo, se=TRUE, col="green")
```
Here w ehave used local regression for the age term with a span of 0.7.
 We can also use lo() function to create interctions before calling the gam() function.
 
We need to have akima pcakage to plot resulting two dimensional surface. 
 
```{r}
gam.lo.i = gam(wage~lo(year,age, span=0.5)+ education, data= Wage)
require(akima)
plot(gam.lo.i)
```
We can fit a logistic regression GAM:

```{r}
gam.lr = gam(I(wage>250)~year+s(age,df=5)+education, data=Wage, family=binomial)
par(mfrow=c(1,3))
plot(gam.lr,se=TRUE, col="green")
```

We can see that there are no high earners in `<HS` category
```{r}
table(education,I(wage>250))
```

So, we try to fit logistic regression GAM leaving this category.

```{r}
gam.lr.s = gam(I(wage>250)~year+s(age, df=5)+education, family=binomial, data=Wage, subset=(education!= "1. < HS Grad"))
plot(gam.lr.s, se=TRUE, col="green")
```